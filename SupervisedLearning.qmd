---
title: "Homework #1: Supervised Learning"
author: "**James Caldwell**"
format: sys6018hw-html
---

```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```



# Required R packages and Directories

```{r packages, message=FALSE, warning=FALSE}
library(tidyverse) # functions for data manipulation
library(ggplot2)

```


# Problem 1: Evaluating a Regression Model

## a. Data generating functions
Create a set of functions to generate data from the following distributions:

\begin{align*}
X &\sim \mathcal{N}(0, 1) \\
Y &= -1 + .5X + .2X^2 + \epsilon \\
\epsilon &\sim \mathcal{N}(0,\, \sigma)
\end{align*}


::: {.callout-note title="Solution"}
```{r}

sim_x <- function(n) rnorm(n)
f <- function(x) -1 + 0.5 * x + 0.2 * x^2
sim_y <- function(x, sd) {
  f(x) + rnorm(length(x), sd = sd)
}

```
:::


## b. Generate training data

Simulate $n=100$ realizations from these distributions using $\sigma=3$. Produce a scatterplot and draw the true regression line $f(x) = E[Y \mid X=x]$.

- Use `set.seed(611)` prior to generating the data.


::: {.callout-note title="Solution"}

```{r}

set.seed(611)

n = 100 # number of observations
sd = 3 # stdev

x = sim_x(n)
y = sim_y(x,sd = sd)

true_regression <- function(x) -1 + 0.5*x + 0.2*(x)^2

data_train <- tibble(x, y)

ggplot(data_train, aes(x,y)) +
  geom_point() + 
  geom_line(aes(y = true_regression(x)), color = "red", linewidth = 1.5)

```

:::


## c. Fit three models

Fit three polynomial regression models using least squares: linear, quadratic, and cubic. Produce another scatterplot, add the fitted lines and true population line $f(x)$  using different colors, and add a legend that maps the line color to a model.

- Note: The true model is quadratic, but we are also fitting linear (less complex) and cubic (more complex) models.

::: {.callout-note title="Solution"}
```{r}

# Fit polynomial regression models
linear_model <- lm(y ~ x, data = data_train)
quadratic_model <- lm(y ~ poly(x, degree = 2), data = data_train)
cubic_model <- lm(y ~ poly(x, degree = 3), data = data_train)

# Predictions
data_train$linear_pred <- predict(linear_model)
data_train$quadratic_pred <- predict(quadratic_model)
data_train$cubic_pred <- predict(cubic_model)

ggplot(data_train, aes(x, y)) +
  geom_point() +
  geom_line(aes(x, linear_pred, color = "Linear"), linewidth = 1) +
  geom_line(aes(x, quadratic_pred, color = "Quadratic"), linewidth = 1) +
  geom_line(aes(x, cubic_pred, color = "Cubic"), linewidth = 1) +
  geom_line(aes(x, -1 + 0.5 * x + 0.2 * x^2, color = "True Population"), linewidth = 1, linetype = "dashed") +
  scale_color_manual(values = c("Linear" = "blue", "Quadratic" = "green", "Cubic" = "red", "True Population" = "black")) +
  labs(title = "Polynomial Regression Models",
       x = "X",
       y = "Y") +
  theme_minimal() +
  theme(legend.position = "top")

```
:::

## d. Predictive performance

Generate a *test data* set of 10,000 observations from the same distributions. Use `set.seed(612)` prior to generating the test data.

- Calculate the estimated mean squared error (MSE) for each model.
- Are the results as expected?

::: {.callout-note title="Solution"}
```{r}

set.seed(612)

n_test = 10000 # number of observations
sd = 3

x = sim_x(n_test)
y = sim_y(x,sd = sd)
data_test = data.frame(x, y)

data_test$linear_pred <- predict(linear_model, newdata = data.frame(x = data_test$x_test))
data_test$quadratic_pred <- predict(quadratic_model, newdata = data.frame(x = data_test$x_test))
data_test$cubic_pred <- predict(cubic_model, newdata = data.frame(x = data_test$x_test))

mse_linear <- mean((data_test$linear_pred - data_test$y)^2)
mse_quadratic <- mean((data_test$quadratic_pred - data_test$y)^2)
mse_cubic <- mean((data_test$cubic_pred - data_test$y)^2)

cat("MSE for Linear Model:", mse_linear, "\n")
cat("MSE for Quadratic Model:", mse_quadratic, "\n")
cat("MSE for Cubic Model:", mse_cubic, "\n")


```

At a glance, the linear model performs the best (it has the lowest MSE). This result is not as expected. The expected best model should be the quadratic model, since the original model for simulation was a quadratic. However, the result is believable since the original data is very spread out (the error term is quite large for generating the train data). Thus, the train data seems to look like a concave down function for the quadratic/cubic functions, although it's actually concave up. Thus, the linear model performed best.

:::

## e. Optimal performance

What is the best achievable MSE? That is, what is the MSE if the true $f(x)$ was used to evaluate the test set? How close does the best method come to achieving the optimum?

::: {.callout-note title="Solution"}
The best achievable MSE would be the variance of the true data. 

MSE_best = Var(Y_True)

```{r}

mse_best <- var(y)

print(mse_best)

```
:::


## f. Replication

The MSE scores obtained in part *d* came from one realization of training data. Here will we explore how much variation there is in the MSE scores by replicating the simulation many times.

- Re-run parts b. and c. (i.e., generate training data and fit models) 100 times.
    - Do not generate new testing data
    - Use `set.seed(613)` prior to running the simulation and do not set the seed in any other places.
- Calculate the test MSE for all simulations.
    - Use the same test data from part d. (This question is only about the variability that comes from the *training data*).
- Create kernel density or histogram plots of the resulting MSE values for each model.lots of the resulting MSE values for each model.

::: {.callout-note title="Solution"}
```{r}

simulate_and_evaluate <- function(data_test,n_train,sd_train) {

  # Simulate training data
  x <- sim_x(n_train)
  y <- sim_y(x, sd = sd_train)
  data_train <- tibble(x, y)

  # Fit regression models
  linear_model <- lm(y ~ x, data = data_train)
  quadratic_model <- lm(y ~ poly(x, 2), data = data_train)
  cubic_model <- lm(y ~ poly(x, 3), data = data_train)
  
  # Predictions on test data
    # I suppressed warnings about newdata predictions and the model
    # I am purposefully making predictions for 10,000 test data points from 100 train datapoints
  data_test$linear_pred <- suppressWarnings({predict(linear_model, newdata = data_test)})
  data_test$quadratic_pred <- suppressWarnings({predict(quadratic_model, newdata = data_test)})
  data_test$cubic_pred <- suppressWarnings({predict(cubic_model, newdata = data_test)})
  
  mse_linear <- mean((data_test$linear_pred - data_test$y)^2)
  mse_quadratic <- mean((data_test$quadratic_pred - data_test$y)^2)
  mse_cubic <- mean((data_test$cubic_pred - data_test$y)^2)
  
  return(c(mse_linear, mse_quadratic, mse_cubic))
}

# Run simulations
  # Test data comes from part d and is not regenerated
set.seed(613)
n_train = 100
sd_train = 3
num_simulations <- 100
results <- replicate(num_simulations, simulate_and_evaluate(data_test,n_train,sd_train))

# Convert results to a data frame
results_df <- data.frame(t(results))

# Set names for columns in results_df
colnames(results_df) <- c("Linear", "Quadratic", "Cubic")

# Gather MSE values into a long format
gathered_results <- gather(results_df, key = "Model", value = "MSE", Linear, Quadratic, Cubic)

# Create histogram plots
ggplot(gathered_results, aes(x = MSE, fill = Model)) +
  geom_histogram(binwidth = 0.25, position = "identity", alpha = 0.7) +
  labs(title = "Distribution of Test MSE for Each Model",
       x = "Test MSE",
       y = "Frequency") +
  theme_minimal()


```
:::

## g. Best model

Show a count of how many times each model was the best. That is, out of the 100 simulations, count how many times each model had the lowest MSE.

::: {.callout-note title="Solution"}
```{r}

# Initialize counts
best_model_counts <- c(Linear = 0, Quadratic = 0, Cubic = 0)

# Loop through each simulation
for (i in 1:num_simulations) {
  # Find the model with the lowest MSE in each simulation
  best_model <- names(results_df)[which.min(results_df[i, ])]
  # Increment the count for the best model
  best_model_counts[best_model] <- best_model_counts[best_model] + 1
}

# Create a data frame to display the counts
counts_df <- data.frame(Model = names(best_model_counts), Count = best_model_counts)

# Print the counts
print(counts_df)


```
:::

## h. Function to implement simulation

Write a function that implements the simulation in part *f*. The function should have arguments for i) the size of the training data $n$, ii) the standard deviation of the random error $\sigma$, and iii) the test data.  Use the same `set.seed(613)`. 

::: {.callout-note title="Solution"}

I decided to adjust the function in f. above to accept n, sd, and the test data. Hopefully this is an acceptable format for the answer to this part: please see function above.

:::


## i. Performance when $\sigma=2$ 

Use your function to repeat the simulation in part *f*, but use $\sigma=2$. Report the number of times each model was best (you do not need to produce any plots). 

- First generate new test data with ($n = 10000$, $\sigma = 2$, using `seed = 612`). 

::: {.callout-note title="Solution"}
```{r}

# Generate new test data
set.seed(612)
n_test = 10000 # number of observations
sd_test = 2
x = sim_x(n_test)
y = sim_y(x,sd = sd_test)
data_test = data.frame(x, y)

# Training data parameters
set.seed(613)
n_train = 100
sd_train = 2
num_simulations <- 100

results <- replicate(num_simulations, simulate_and_evaluate(data_test,n_train,sd_train))

results_df <- data.frame(t(results))

# Set names for columns in results_df
colnames(results_df) <- c("Linear", "Quadratic", "Cubic")

best_model_counts <- c(Linear = 0, Quadratic = 0, Cubic = 0)

# Loop through each simulation
for (i in 1:num_simulations) {
  # Find the model with the lowest MSE in each simulation
  best_model <- names(results_df)[which.min(results_df[i, ])]
  # Increment the count for the best model
  best_model_counts[best_model] <- best_model_counts[best_model] + 1
}

# Create a data frame to display the counts
counts_df <- data.frame(Model = names(best_model_counts), Count = best_model_counts)

# Print the counts
print(counts_df)

```
:::

## j. Performance when $\sigma=4$ and $n=300$

Repeat *i*, but now use $\sigma=4$ and $n=300$.

- First generate new test data with ($n = 10000$, $\sigma = 4$, using `seed = 612`). 

::: {.callout-note title="Solution"}
```{r}

# Generate new test data
set.seed(612)
n_test = 10000 # number of observations
sd_test = 4
x = sim_x(n_test)
y = sim_y(x,sd = sd_test)
data_test = data.frame(x, y)

# Training data parameters
set.seed(613)
n_train = 300 # For some reason I get an error when I input 300. I know this should be 300
sd_train = 4
num_simulations <- 100

results <- replicate(num_simulations, simulate_and_evaluate(data_test,n_train,sd_train))

results_df <- data.frame(t(results))

# Set names for columns in results_df
colnames(results_df) <- c("Linear", "Quadratic", "Cubic")

best_model_counts <- c(Linear = 0, Quadratic = 0, Cubic = 0)

# Loop through each simulation
for (i in 1:num_simulations) {
  # Find the model with the lowest MSE in each simulation
  best_model <- names(results_df)[which.min(results_df[i, ])]
  # Increment the count for the best model
  best_model_counts[best_model] <- best_model_counts[best_model] + 1
}

# Create a data frame to display the counts
counts_df <- data.frame(Model = names(best_model_counts), Count = best_model_counts)

# Print the counts
print(counts_df)

```
:::


## k. Understanding

Describe the effects $\sigma$ and $n$ has on selection of the best model? Why is the *true* model form (i.e., quadratic) not always the *best* model to use when prediction is the goal?

::: {.callout-note title="Solution"}

Increasing $\sigma$ increases the spread within the original data. Thus, increasing $\sigma$ then makes the data look less and less like the original function used to generate the data. This allows for other model forms (ex: a linear model) to more accurately describe the data than the true model form (ex: a quadratic). 

$n$ has a less pronounced effect than $\sigma$ does, however increasing $n$ generally allows for a more accurate "picture" of the data for a model to use for interpreting trends. Thus, increasing $n$ should lead towards a model that is closer to the true data form. However, after a certain point, the data shows enough of a picture and increasing $n$ probably does nothing more than add more noise to the data. 

The true model form is not always the goal for a prediction because we want to model the actual data where it actually lies. Large variation (large $\sigma$), zoomed in range of data, and other factors can distort the data enough that there may be a model that describes the data better than the true model form.

:::

